{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "45071f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        # 1 input image channel, 6 output channels, 5x5 square convolution\n",
    "        # kernel\n",
    "        self.conv1 = nn.Conv2d(1, 6, 5)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        # an affine operation: y = Wx + b\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)  # 5*5 from image dimension\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "        self.predprob = 0\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Max pooling over a (2, 2) window\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
    "        # If the size is a square, you can specify with a single number\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
    "        x = torch.flatten(x, 1) # flatten all dimensions except the batch dimension\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        self.predprob = x\n",
    "        return (x > 0.5).type()\n",
    "\n",
    "\n",
    "#net = Net()\n",
    "#print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a5c6a548",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('train.csv')\n",
    "dfTest = pd.read_csv('test.csv')\n",
    "dataset = pd.read_csv('train.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d3fdc43c",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert dataset.isnull().sum().sum() == 0\n",
    "\n",
    "num_col_names = list(dataset.select_dtypes(include='number').columns)\n",
    "cat_col_names = list(set(dataset.columns) - set(num_col_names))\n",
    "\n",
    "# 70:15:15 stratified split\n",
    "from sklearn.model_selection import train_test_split\n",
    "train, tmp = train_test_split(dataset, test_size=0.3, stratify = dataset[\"Attrition\"], random_state=42)\n",
    "val, test  = train_test_split(tmp,     test_size=0.5, stratify =     tmp[\"Attrition\"], random_state=42)\n",
    "\n",
    "\n",
    "#!pip install -q pytorch_tabular[extra]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0f11cc25",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -q pytorch_tabular[extra]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "70911a7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Department',\n",
       " 'Over18',\n",
       " 'OverTime',\n",
       " 'Gender',\n",
       " 'JobRole',\n",
       " 'BusinessTravel',\n",
       " 'EducationField',\n",
       " 'MaritalStatus']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat_col_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "944723ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       0\n",
       "1       0\n",
       "2       0\n",
       "3       0\n",
       "4       1\n",
       "       ..\n",
       "1672    0\n",
       "1673    0\n",
       "1674    1\n",
       "1675    0\n",
       "1676    0\n",
       "Name: Attrition, Length: 1677, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"Attrition\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74110ff1",
   "metadata": {},
   "source": [
    "Help from chatgpt:\n",
    "\n",
    "You can convert a pandas dataframe with categorical data fields to a PyTorch tensor using the torch.from_numpy() function and the .values attribute of the dataframe.\n",
    "\n",
    "First, you will need to convert the categorical data fields to numerical values using the .cat.codes attribute of the dataframe column, or using pandas' get_dummies() function.\n",
    "\n",
    "Then you can use the .values attribute to extract the numpy array from the dataframe and pass it to the torch.from_numpy() function to convert it to a PyTorch tensor.\n",
    "\n",
    "Example:\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "df = pd.DataFrame({'A': ['a', 'b', 'a'], 'B': [1, 2, 3]})\n",
    "df = pd.get_dummies(df, columns=['A'])\n",
    "tensor = torch.from_numpy(df.values)\n",
    "\n",
    "Note that in the above example we use the pd.get_dummies function to convert the categorical column 'A' to multiple columns, one for each category in the column.\n",
    "\n",
    "Thanks A.I.\n",
    "\n",
    "You're welcome! Let me know if you have any other questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0ea428d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = pd.get_dummies(df, columns=cat_col_names)\n",
    "tensor = torch.from_numpy(df.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "997b35d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       0\n",
       "1       0\n",
       "2       0\n",
       "3       1\n",
       "4       1\n",
       "       ..\n",
       "1672    1\n",
       "1673    0\n",
       "1674    0\n",
       "1675    0\n",
       "1676    1\n",
       "Name: Gender_Female, Length: 1677, dtype: uint8"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"Gender_Female\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5093dfa8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1677, 56])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1f766e67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   0,   36,  599,  ...,    0,    1,    0],\n",
       "        [   1,   35,  921,  ...,    0,    1,    0],\n",
       "        [   2,   32,  718,  ...,    1,    0,    0],\n",
       "        ...,\n",
       "        [1674,   29, 1184,  ...,    0,    1,    0],\n",
       "        [1675,   36,  441,  ...,    1,    0,    0],\n",
       "        [1676,   36, 1141,  ...,    0,    0,    1]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d2c4126",
   "metadata": {},
   "source": [
    "how to split dataset into train and test set in pytorch. and then how to use dataloader with the result?\n",
    "\n",
    "In PyTorch, you can split a dataset into a training set and a test set using the torch.utils.data.random_split() function. This function takes in a dataset and a list of lengths, and splits the dataset into multiple subsets with the specified lengths.\n",
    "\n",
    "For example, to split a dataset into a training set and a test set, you can use the following code:\n",
    "\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "dataset = ... # your dataset\n",
    "train_ratio = 0.8 # ratio of data to be used for training\n",
    "\n",
    "train_len = int(train_ratio * len(dataset))\n",
    "test_len = len(dataset) - train_len\n",
    "\n",
    "train_set, test_set = random_split(dataset, [train_len, test_len])\n",
    "\n",
    "Once you have the training and test sets, you can use the DataLoader class from torch.utils.data to load the data in batches.\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "In this example, we pass the train_set and test_set as input to the DataLoader, also we set the batch size to 32, and shuffle to True for the training set, and False for the test set.\n",
    "\n",
    "You can then use these data loaders to iterate through the training and test sets in batches. For example, you can use a for loop to iterate through the training data in batches:\n",
    "\n",
    "for data, labels in train_loader:\n",
    "    # do something with the data and labels\n",
    "\n",
    "This way, the data is loaded in batches of size \"batch_size\" and can be used for training your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d5eec0d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df\n",
    "labels = df.pop(\"Attrition\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6c8e3cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import random_split\n",
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "sca = StandardScaler()\n",
    "\n",
    "df = sca.fit_transform(df)\n",
    "# X_test = sca.fit_transform(X_test)\n",
    "# test = sca.fit_transform(test)\n",
    "\n",
    "\n",
    "tensor = torch.from_numpy(df)\n",
    "labeltensor = torch.from_numpy(labels.values)\n",
    "\n",
    "dataset = TensorDataset(tensor, labeltensor)\n",
    "\n",
    "\n",
    "\n",
    "train_ratio = 0.85 # ratio of data to be used for training\n",
    "\n",
    "train_len = int(train_ratio * len(dataset))\n",
    "test_len = len(dataset) - train_len\n",
    "\n",
    "train_set, test_set = random_split(dataset, [train_len, test_len])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c750c618",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataset.Subset at 0x1becaf0fc40>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6cf339b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loader.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dd6ec5a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "loader = DataLoader(train_set, batch_size=32, shuffle=True)\n",
    "loaderVal = DataLoader(test_set, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "648852d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fd81c6e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MihaNetForSwag(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(MihaNetForSwag, self).__init__()\n",
    "        # 1 input image channel, 6 output channels, 5x5 square convolution\n",
    "        # an affine operation: y = Wx + b\n",
    "        self.fc1 = nn.Linear(55, 256)  # 5*5 from image dimension\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.fc3 = nn.Linear(128, 128)\n",
    "        self.fc3 = nn.Linear(128, 128)\n",
    "        self.fc3 = nn.Linear(128, 64)\n",
    "        self.fc4 = nn.Linear(64, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Max pooling over a (2, 2) window\n",
    "        # x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
    "        # If the size is a square, you can specify with a single number\n",
    "        # x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
    "        # x = torch.flatten(x, 1) # flatten all dimensions except the batch dimension\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = torch.sigmoid(self.fc4(x))\n",
    "        #print(x.shape)\n",
    "        #print(x)\n",
    "        # x = self.fc4(x)\n",
    "        return x\n",
    "    \n",
    "\n",
    "class MihaNetForSwager(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(MihaNetForSwag, self).__init__()\n",
    "        # 1 input image channel, 6 output channels, 5x5 square convolution\n",
    "        # an affine operation: y = Wx + b\n",
    "        self.fc1 = nn.Linear(55, 256)  # 5*5 from image dimension\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.fc3 = nn.Linear(128, 64)\n",
    "        self.fc4 = nn.Linear(64, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Max pooling over a (2, 2) window\n",
    "        # x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
    "        # If the size is a square, you can specify with a single number\n",
    "        # x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
    "        # x = torch.flatten(x, 1) # flatten all dimensions except the batch dimension\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = torch.sigmoid(self.fc4(x))\n",
    "        #print(x.shape)\n",
    "        #print(x)\n",
    "        # x = self.fc4(x)\n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6912db5c",
   "metadata": {},
   "source": [
    "\n",
    "## VARIABLES\n",
    "\n",
    "# Epochs to train for\n",
    "epochs = 50\n",
    "# batch size \n",
    "batch_size = 128\n",
    "\n",
    "# Length of sequence\n",
    "seq_len = 100\n",
    "\n",
    "# for printing report purposes\n",
    "# always start at 0\n",
    "tracker = 0\n",
    "\n",
    "# number of characters in text\n",
    "# num_char = max(encoded_text)+1\n",
    "\n",
    "\n",
    "\n",
    "model = MihaNetForSwag()\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=0.001)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "\n",
    "\n",
    "model.train()\n",
    "\n",
    "\n",
    "\n",
    "# Check to see if using GPU\n",
    "#if model.use_gpu:\n",
    "#    model.cuda()\n",
    "\n",
    "for i in range(epochs):\n",
    "    \n",
    "    # hidden = model.hidden_state(batch_size)\n",
    "    \n",
    "    # print(i)\n",
    "    loader = iter(DataLoader(train_set, batch_size=batch_size, shuffle=True))\n",
    "    \n",
    "    for x,y in loader:\n",
    "        \n",
    "        # print(x)\n",
    "        \n",
    "        tracker += 1\n",
    "        \n",
    "        # One Hot Encode incoming data\n",
    "        # x = loader.next() # one_hot_encoder(x,num_char)\n",
    "        \n",
    "        # Convert Numpy Arrays to Tensor\n",
    "        \n",
    "        inputs = x.type(torch.FloatTensor)\n",
    "        #torch.from_numpy(x)\n",
    "        targets = y.type(torch.FloatTensor) #torch.from_numpy(y)\n",
    "        \n",
    "        # Adjust for GPU if necessary\n",
    "        \n",
    "        # if model.use_gpu:\n",
    "        #    inputs = inputs.cuda()\n",
    "        #    targets = targets.cuda()\n",
    "            \n",
    "        # Reset Hidden State\n",
    "        # If we dont' reset we would backpropagate through all training history\n",
    "        # hidden = tuple([state.data for state in hidden])\n",
    "        \n",
    "        model.zero_grad()\n",
    "        \n",
    "        output = model.forward(inputs)\n",
    "        loss = criterion(output, targets)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        # POSSIBLE EXPLODING GRADIENT PROBLEM!\n",
    "        # LET\"S CLIP JUST IN CASE\n",
    "        nn.utils.clip_grad_norm_(model.parameters(),max_norm=5)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        \n",
    "        \n",
    "        ###################################\n",
    "        ### CHECK ON VALIDATION SET ######\n",
    "        #################################\n",
    "        \n",
    "        \n",
    "        \n",
    "        if tracker % 2 == 0:\n",
    "            loaderVal = iter(DataLoader(test_set, batch_size=batch_size, shuffle=True))\n",
    "            # val_hidden = model.hidden_state(batch_size)\n",
    "            val_losses = []\n",
    "            model.eval()\n",
    "            \n",
    "            for x,y in loaderVal:\n",
    "                \n",
    "                # One Hot Encode incoming data\n",
    "                # x = one_hot_encoder(x,num_char)\n",
    "                \n",
    "\n",
    "                # Convert Numpy Arrays to Tensor\n",
    "\n",
    "                inputs = x.type(torch.FloatTensor) #torch.from_numpy(x)\n",
    "                targets = y.type(torch.FloatTensor) #torch.from_numpy(y)\n",
    "\n",
    "                # Adjust for GPU if necessary\n",
    "\n",
    "                #if model.use_gpu:\n",
    "\n",
    "                #    inputs = inputs.cuda()\n",
    "                #    targets = targets.cuda()\n",
    "                    \n",
    "                # Reset Hidden State\n",
    "                # If we dont' reset we would backpropagate through \n",
    "                # all training history\n",
    "                # val_hidden = tuple([state.data for state in val_hidden])\n",
    "                \n",
    "                output = model.forward(inputs)\n",
    "                val_loss = criterion(output, targets)\n",
    "        \n",
    "                val_losses.append(val_loss.item())\n",
    "            \n",
    "            \n",
    "            # Reset to training model after val for loop\n",
    "            model.train()\n",
    "            \n",
    "            print(f\"Epoch: {i} Step: {tracker} Val Loss: {val_loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7e863648",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Loss: 0.3043\n",
      "Epoch 2/100, Loss: 0.1723\n",
      "Epoch 3/100, Loss: 0.1338\n",
      "Epoch 4/100, Loss: 0.3010\n",
      "Epoch 5/100, Loss: 0.2930\n",
      "Epoch 6/100, Loss: 0.0732\n",
      "Epoch 7/100, Loss: 0.1071\n",
      "Epoch 8/100, Loss: 0.2028\n",
      "Epoch 9/100, Loss: 0.1179\n",
      "Epoch 10/100, Loss: 0.0661\n",
      "Epoch 11/100, Loss: 0.1149\n",
      "Epoch 12/100, Loss: 0.0878\n",
      "Epoch 13/100, Loss: 0.2119\n",
      "Epoch 14/100, Loss: 0.0039\n",
      "Epoch 15/100, Loss: 0.0119\n",
      "Epoch 16/100, Loss: 0.0014\n",
      "Epoch 17/100, Loss: 0.0003\n",
      "Epoch 18/100, Loss: 0.0003\n",
      "Epoch 19/100, Loss: 0.0007\n",
      "Epoch 20/100, Loss: 0.0000\n",
      "Epoch 21/100, Loss: 0.0004\n",
      "Epoch 22/100, Loss: 0.0002\n",
      "Epoch 23/100, Loss: 0.0002\n",
      "Epoch 24/100, Loss: 0.0003\n",
      "Epoch 25/100, Loss: 0.0004\n",
      "Epoch 26/100, Loss: 0.0006\n",
      "Epoch 27/100, Loss: 0.0002\n",
      "Epoch 28/100, Loss: 0.0000\n",
      "Epoch 29/100, Loss: 0.0001\n",
      "Epoch 30/100, Loss: 0.0005\n",
      "Epoch 31/100, Loss: 0.0002\n",
      "Epoch 32/100, Loss: 0.0002\n",
      "Epoch 33/100, Loss: 0.0006\n",
      "Epoch 34/100, Loss: 0.0001\n",
      "Epoch 35/100, Loss: 0.0000\n",
      "Epoch 36/100, Loss: 0.0002\n",
      "Epoch 37/100, Loss: 0.0003\n",
      "Epoch 38/100, Loss: 0.0001\n",
      "Epoch 39/100, Loss: 0.0001\n",
      "Epoch 40/100, Loss: 0.0001\n",
      "Epoch 41/100, Loss: 0.0001\n",
      "Epoch 42/100, Loss: 0.0001\n",
      "Epoch 43/100, Loss: 0.0001\n",
      "Epoch 44/100, Loss: 0.0000\n",
      "Epoch 45/100, Loss: 0.0001\n",
      "Epoch 46/100, Loss: 0.0001\n",
      "Epoch 47/100, Loss: 0.0001\n",
      "Epoch 48/100, Loss: 0.0001\n",
      "Epoch 49/100, Loss: 0.0000\n",
      "Epoch 50/100, Loss: 0.0000\n",
      "Epoch 51/100, Loss: 0.0000\n",
      "Epoch 52/100, Loss: 0.0001\n",
      "Epoch 53/100, Loss: 0.0000\n",
      "Epoch 54/100, Loss: 0.0000\n",
      "Epoch 55/100, Loss: 0.0000\n",
      "Epoch 56/100, Loss: 0.0001\n",
      "Epoch 57/100, Loss: 0.0000\n",
      "Epoch 58/100, Loss: 0.0001\n",
      "Epoch 59/100, Loss: 0.0000\n",
      "Epoch 60/100, Loss: 0.0000\n",
      "Epoch 61/100, Loss: 0.0000\n",
      "Epoch 62/100, Loss: 0.0000\n",
      "Epoch 63/100, Loss: 0.0000\n",
      "Epoch 64/100, Loss: 0.0000\n",
      "Epoch 65/100, Loss: 0.0001\n",
      "Epoch 66/100, Loss: 0.0000\n",
      "Epoch 67/100, Loss: 0.0000\n",
      "Epoch 68/100, Loss: 0.0000\n",
      "Epoch 69/100, Loss: 0.0000\n",
      "Epoch 70/100, Loss: 0.0000\n",
      "Epoch 71/100, Loss: 0.0000\n",
      "Epoch 72/100, Loss: 0.0000\n",
      "Epoch 73/100, Loss: 0.0000\n",
      "Epoch 74/100, Loss: 0.0000\n",
      "Epoch 75/100, Loss: 0.0000\n",
      "Epoch 76/100, Loss: 0.0000\n",
      "Epoch 77/100, Loss: 0.0000\n",
      "Epoch 78/100, Loss: 0.0000\n",
      "Epoch 79/100, Loss: 0.0000\n",
      "Epoch 80/100, Loss: 0.0000\n",
      "Epoch 81/100, Loss: 0.0000\n",
      "Epoch 82/100, Loss: 0.0000\n",
      "Epoch 83/100, Loss: 0.0000\n",
      "Epoch 84/100, Loss: 0.0000\n",
      "Epoch 85/100, Loss: 0.0000\n",
      "Epoch 86/100, Loss: 0.0000\n",
      "Epoch 87/100, Loss: 0.0000\n",
      "Epoch 88/100, Loss: 0.0000\n",
      "Epoch 89/100, Loss: 0.0000\n",
      "Epoch 90/100, Loss: 0.0000\n",
      "Epoch 91/100, Loss: 0.0000\n",
      "Epoch 92/100, Loss: 0.0000\n",
      "Epoch 93/100, Loss: 0.0000\n",
      "Epoch 94/100, Loss: 0.0000\n",
      "Epoch 95/100, Loss: 0.0000\n",
      "Epoch 96/100, Loss: 0.0000\n",
      "Epoch 97/100, Loss: 0.0000\n",
      "Epoch 98/100, Loss: 0.0000\n",
      "Epoch 99/100, Loss: 0.0000\n",
      "Epoch 100/100, Loss: 0.0000\n"
     ]
    }
   ],
   "source": [
    "# set the number of training iterations (epochs)\n",
    "num_epochs = 100\n",
    "#torch.set_grad_enabled(True) \n",
    "\n",
    "model = MihaNetForSwag()\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=0.001)\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "# start the training loop\n",
    "for epoch in range(num_epochs):\n",
    "    # loop over the training data in batches\n",
    "    for data, labels in loader:\n",
    "        # pass the data through the model\n",
    "        output = model(data.type(torch.FloatTensor))\n",
    "        # calculate the loss\n",
    "        #print(output)\n",
    "        #print(labels)\n",
    "        \n",
    "        #size = 32\n",
    "        #print(output.shape)\n",
    "        #print(labels.shape)\n",
    "        #print(\"$$$\")\n",
    "        \n",
    "        #print(output)\n",
    "        #print((output > 0.5).type(torch.FloatTensor))\n",
    "        #print(labels)\n",
    "        \n",
    "        #print(output)\n",
    "        #print(labels.reshape(labels.size()[0],1))\n",
    "        # loss = criterion(output.reshape(output.size()[0]), labels.type(torch.FloatTensor))\n",
    "        loss = criterion(output, labels.reshape(-1,1).type(torch.FloatTensor))\n",
    "        # zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "        # perform backpropagation\n",
    "        loss.backward()\n",
    "        # update the model's parameters\n",
    "        optimizer.step()\n",
    "\n",
    "    # print the loss at the end of the epoch\n",
    "    print(\"Epoch {}/{}, Loss: {:.4f}\".format(epoch+1, num_epochs, loss.item()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
