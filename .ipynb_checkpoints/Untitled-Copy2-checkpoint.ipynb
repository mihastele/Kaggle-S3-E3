{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "45071f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        # 1 input image channel, 6 output channels, 5x5 square convolution\n",
    "        # kernel\n",
    "        self.conv1 = nn.Conv2d(1, 6, 5)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        # an affine operation: y = Wx + b\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)  # 5*5 from image dimension\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "        self.predprob = 0\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Max pooling over a (2, 2) window\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
    "        # If the size is a square, you can specify with a single number\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
    "        x = torch.flatten(x, 1) # flatten all dimensions except the batch dimension\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        self.predprob = x\n",
    "        return (x > 0.5).type()\n",
    "\n",
    "\n",
    "#net = Net()\n",
    "#print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a5c6a548",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('train.csv')\n",
    "dfTest = pd.read_csv('test.csv')\n",
    "dataset = pd.read_csv('train.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d3fdc43c",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert dataset.isnull().sum().sum() == 0\n",
    "\n",
    "num_col_names = list(dataset.select_dtypes(include='number').columns)\n",
    "cat_col_names = list(set(dataset.columns) - set(num_col_names))\n",
    "\n",
    "# 70:15:15 stratified split\n",
    "from sklearn.model_selection import train_test_split\n",
    "train, tmp = train_test_split(dataset, test_size=0.3, stratify = dataset[\"Attrition\"], random_state=42)\n",
    "val, test  = train_test_split(tmp,     test_size=0.5, stratify =     tmp[\"Attrition\"], random_state=42)\n",
    "\n",
    "\n",
    "#!pip install -q pytorch_tabular[extra]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0f11cc25",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -q pytorch_tabular[extra]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "70911a7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['JobRole',\n",
       " 'OverTime',\n",
       " 'MaritalStatus',\n",
       " 'Department',\n",
       " 'EducationField',\n",
       " 'Over18',\n",
       " 'Gender',\n",
       " 'BusinessTravel']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat_col_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "14f0ff4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       0\n",
       "1       0\n",
       "2       0\n",
       "3       0\n",
       "4       1\n",
       "       ..\n",
       "1672    0\n",
       "1673    0\n",
       "1674    1\n",
       "1675    0\n",
       "1676    0\n",
       "Name: Attrition, Length: 1677, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"Attrition\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74110ff1",
   "metadata": {},
   "source": [
    "Help from chatgpt:\n",
    "\n",
    "You can convert a pandas dataframe with categorical data fields to a PyTorch tensor using the torch.from_numpy() function and the .values attribute of the dataframe.\n",
    "\n",
    "First, you will need to convert the categorical data fields to numerical values using the .cat.codes attribute of the dataframe column, or using pandas' get_dummies() function.\n",
    "\n",
    "Then you can use the .values attribute to extract the numpy array from the dataframe and pass it to the torch.from_numpy() function to convert it to a PyTorch tensor.\n",
    "\n",
    "Example:\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "df = pd.DataFrame({'A': ['a', 'b', 'a'], 'B': [1, 2, 3]})\n",
    "df = pd.get_dummies(df, columns=['A'])\n",
    "tensor = torch.from_numpy(df.values)\n",
    "\n",
    "Note that in the above example we use the pd.get_dummies function to convert the categorical column 'A' to multiple columns, one for each category in the column.\n",
    "\n",
    "Thanks A.I.\n",
    "\n",
    "You're welcome! Let me know if you have any other questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0ea428d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = pd.get_dummies(df, columns=cat_col_names)\n",
    "tensor = torch.from_numpy(df.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "997b35d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       0\n",
       "1       0\n",
       "2       0\n",
       "3       1\n",
       "4       1\n",
       "       ..\n",
       "1672    1\n",
       "1673    0\n",
       "1674    0\n",
       "1675    0\n",
       "1676    1\n",
       "Name: Gender_Female, Length: 1677, dtype: uint8"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"Gender_Female\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5093dfa8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1677, 56])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1f766e67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   0,   36,  599,  ...,    0,    1,    0],\n",
       "        [   1,   35,  921,  ...,    0,    0,    1],\n",
       "        [   2,   32,  718,  ...,    0,    0,    1],\n",
       "        ...,\n",
       "        [1674,   29, 1184,  ...,    0,    1,    0],\n",
       "        [1675,   36,  441,  ...,    0,    0,    1],\n",
       "        [1676,   36, 1141,  ...,    0,    0,    1]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d2c4126",
   "metadata": {},
   "source": [
    "how to split dataset into train and test set in pytorch. and then how to use dataloader with the result?\n",
    "\n",
    "In PyTorch, you can split a dataset into a training set and a test set using the torch.utils.data.random_split() function. This function takes in a dataset and a list of lengths, and splits the dataset into multiple subsets with the specified lengths.\n",
    "\n",
    "For example, to split a dataset into a training set and a test set, you can use the following code:\n",
    "\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "dataset = ... # your dataset\n",
    "train_ratio = 0.8 # ratio of data to be used for training\n",
    "\n",
    "train_len = int(train_ratio * len(dataset))\n",
    "test_len = len(dataset) - train_len\n",
    "\n",
    "train_set, test_set = random_split(dataset, [train_len, test_len])\n",
    "\n",
    "Once you have the training and test sets, you can use the DataLoader class from torch.utils.data to load the data in batches.\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "In this example, we pass the train_set and test_set as input to the DataLoader, also we set the batch size to 32, and shuffle to True for the training set, and False for the test set.\n",
    "\n",
    "You can then use these data loaders to iterate through the training and test sets in batches. For example, you can use a for loop to iterate through the training data in batches:\n",
    "\n",
    "for data, labels in train_loader:\n",
    "    # do something with the data and labels\n",
    "\n",
    "This way, the data is loaded in batches of size \"batch_size\" and can be used for training your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d5eec0d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df\n",
    "labels = df.pop(\"Attrition\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2bf74409",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import random_split\n",
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "tensor = torch.from_numpy(df.values)\n",
    "labeltensor = torch.from_numpy(labels.values)\n",
    "\n",
    "dataset = TensorDataset(tensor, labeltensor)\n",
    "\n",
    "\n",
    "\n",
    "train_ratio = 0.85 # ratio of data to be used for training\n",
    "\n",
    "train_len = int(train_ratio * len(dataset))\n",
    "test_len = len(dataset) - train_len\n",
    "\n",
    "train_set, test_set = random_split(dataset, [train_len, test_len])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c750c618",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataset.Subset at 0x2153aa3c8e0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6cf339b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loader.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dd6ec5a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "loader = DataLoader(train_set, batch_size=32, shuffle=True)\n",
    "loaderVal = DataLoader(test_set, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "648852d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fd81c6e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MihaNetForSwag(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(MihaNetForSwag, self).__init__()\n",
    "        # 1 input image channel, 6 output channels, 5x5 square convolution\n",
    "        # an affine operation: y = Wx + b\n",
    "        self.fc1 = nn.Linear(55, 256)  # 5*5 from image dimension\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.fc3 = nn.Linear(128, 128)\n",
    "        self.fc3 = nn.Linear(128, 128)\n",
    "        self.fc3 = nn.Linear(128, 64)\n",
    "        self.fc4 = nn.Linear(64, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Max pooling over a (2, 2) window\n",
    "        # x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
    "        # If the size is a square, you can specify with a single number\n",
    "        # x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
    "        # x = torch.flatten(x, 1) # flatten all dimensions except the batch dimension\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = torch.sigmoid(self.fc4(x))\n",
    "        #print(x.shape)\n",
    "        #print(x)\n",
    "        # x = self.fc4(x)\n",
    "        return x\n",
    "    \n",
    "\n",
    "class MihaNetForSwager(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(MihaNetForSwag, self).__init__()\n",
    "        # 1 input image channel, 6 output channels, 5x5 square convolution\n",
    "        # an affine operation: y = Wx + b\n",
    "        self.fc1 = nn.Linear(55, 256)  # 5*5 from image dimension\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.fc3 = nn.Linear(128, 64)\n",
    "        self.fc4 = nn.Linear(64, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Max pooling over a (2, 2) window\n",
    "        # x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
    "        # If the size is a square, you can specify with a single number\n",
    "        # x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
    "        # x = torch.flatten(x, 1) # flatten all dimensions except the batch dimension\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = torch.sigmoid(self.fc4(x))\n",
    "        #print(x.shape)\n",
    "        #print(x)\n",
    "        # x = self.fc4(x)\n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9f726ff",
   "metadata": {},
   "source": [
    "\n",
    "## VARIABLES\n",
    "\n",
    "# Epochs to train for\n",
    "epochs = 50\n",
    "# batch size \n",
    "batch_size = 128\n",
    "\n",
    "# Length of sequence\n",
    "seq_len = 100\n",
    "\n",
    "# for printing report purposes\n",
    "# always start at 0\n",
    "tracker = 0\n",
    "\n",
    "# number of characters in text\n",
    "# num_char = max(encoded_text)+1\n",
    "\n",
    "\n",
    "\n",
    "model = MihaNetForSwag()\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=0.001)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "\n",
    "\n",
    "model.train()\n",
    "\n",
    "\n",
    "\n",
    "# Check to see if using GPU\n",
    "#if model.use_gpu:\n",
    "#    model.cuda()\n",
    "\n",
    "for i in range(epochs):\n",
    "    \n",
    "    # hidden = model.hidden_state(batch_size)\n",
    "    \n",
    "    # print(i)\n",
    "    loader = iter(DataLoader(train_set, batch_size=batch_size, shuffle=True))\n",
    "    \n",
    "    for x,y in loader:\n",
    "        \n",
    "        # print(x)\n",
    "        \n",
    "        tracker += 1\n",
    "        \n",
    "        # One Hot Encode incoming data\n",
    "        # x = loader.next() # one_hot_encoder(x,num_char)\n",
    "        \n",
    "        # Convert Numpy Arrays to Tensor\n",
    "        \n",
    "        inputs = x.type(torch.FloatTensor)\n",
    "        #torch.from_numpy(x)\n",
    "        targets = y.type(torch.FloatTensor) #torch.from_numpy(y)\n",
    "        \n",
    "        # Adjust for GPU if necessary\n",
    "        \n",
    "        # if model.use_gpu:\n",
    "        #    inputs = inputs.cuda()\n",
    "        #    targets = targets.cuda()\n",
    "            \n",
    "        # Reset Hidden State\n",
    "        # If we dont' reset we would backpropagate through all training history\n",
    "        # hidden = tuple([state.data for state in hidden])\n",
    "        \n",
    "        model.zero_grad()\n",
    "        \n",
    "        output = model.forward(inputs)\n",
    "        loss = criterion(output, targets)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        # POSSIBLE EXPLODING GRADIENT PROBLEM!\n",
    "        # LET\"S CLIP JUST IN CASE\n",
    "        nn.utils.clip_grad_norm_(model.parameters(),max_norm=5)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        \n",
    "        \n",
    "        ###################################\n",
    "        ### CHECK ON VALIDATION SET ######\n",
    "        #################################\n",
    "        \n",
    "        \n",
    "        \n",
    "        if tracker % 2 == 0:\n",
    "            loaderVal = iter(DataLoader(test_set, batch_size=batch_size, shuffle=True))\n",
    "            # val_hidden = model.hidden_state(batch_size)\n",
    "            val_losses = []\n",
    "            model.eval()\n",
    "            \n",
    "            for x,y in loaderVal:\n",
    "                \n",
    "                # One Hot Encode incoming data\n",
    "                # x = one_hot_encoder(x,num_char)\n",
    "                \n",
    "\n",
    "                # Convert Numpy Arrays to Tensor\n",
    "\n",
    "                inputs = x.type(torch.FloatTensor) #torch.from_numpy(x)\n",
    "                targets = y.type(torch.FloatTensor) #torch.from_numpy(y)\n",
    "\n",
    "                # Adjust for GPU if necessary\n",
    "\n",
    "                #if model.use_gpu:\n",
    "\n",
    "                #    inputs = inputs.cuda()\n",
    "                #    targets = targets.cuda()\n",
    "                    \n",
    "                # Reset Hidden State\n",
    "                # If we dont' reset we would backpropagate through \n",
    "                # all training history\n",
    "                # val_hidden = tuple([state.data for state in val_hidden])\n",
    "                \n",
    "                output = model.forward(inputs)\n",
    "                val_loss = criterion(output, targets)\n",
    "        \n",
    "                val_losses.append(val_loss.item())\n",
    "            \n",
    "            \n",
    "            # Reset to training model after val for loop\n",
    "            model.train()\n",
    "            \n",
    "            print(f\"Epoch: {i} Step: {tracker} Val Loss: {val_loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b897f248",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "element 0 of tensors does not require grad and does not have a grad_fn",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[1;32mIn [25]\u001b[0m, in \u001b[0;36m<cell line: 10>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     30\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m     31\u001b[0m \u001b[38;5;66;03m# perform backpropagation\u001b[39;00m\n\u001b[1;32m---> 32\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;66;03m# update the model's parameters\u001b[39;00m\n\u001b[0;32m     34\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\torch\\tensor.py:245\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    236\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    237\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    238\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    239\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    243\u001b[0m         create_graph\u001b[38;5;241m=\u001b[39mcreate_graph,\n\u001b[0;32m    244\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs)\n\u001b[1;32m--> 245\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\torch\\autograd\\__init__.py:145\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    142\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m retain_graph \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    143\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m--> 145\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    146\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    147\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: element 0 of tensors does not require grad and does not have a grad_fn"
     ]
    }
   ],
   "source": [
    "# set the number of training iterations (epochs)\n",
    "num_epochs = 100\n",
    "\n",
    "\n",
    "model = MihaNetForSwag()\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=0.001)\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "# start the training loop\n",
    "for epoch in range(num_epochs):\n",
    "    # loop over the training data in batches\n",
    "    for data, labels in loader:\n",
    "        # pass the data through the model\n",
    "        output = model(data.type(torch.FloatTensor))\n",
    "        # calculate the loss\n",
    "        #print(output)\n",
    "        #print(labels)\n",
    "        \n",
    "        #size = 32\n",
    "        #print(output.shape)\n",
    "        #print(labels.shape)\n",
    "        #print(\"$$$\")\n",
    "        \n",
    "        #print(output)\n",
    "        #print((output > 0.5).type(torch.FloatTensor))\n",
    "        #print(labels)\n",
    "        # loss = criterion(output.reshape(output.size()[0]), labels.type(torch.FloatTensor))\n",
    "        loss = criterion(output.reshape(output.size()[0]), labels.type(torch.FloatTensor))\n",
    "        # zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "        # perform backpropagation\n",
    "        loss.backward()\n",
    "        # update the model's parameters\n",
    "        optimizer.step()\n",
    "\n",
    "    # print the loss at the end of the epoch\n",
    "    print(\"Epoch {}/{}, Loss: {:.4f}\".format(epoch+1, num_epochs, loss.item()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
